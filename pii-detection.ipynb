{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7526248,"sourceType":"datasetVersion","datasetId":4308295},{"sourceId":7626226,"sourceType":"datasetVersion","datasetId":4442914},{"sourceId":6064,"sourceType":"modelInstanceVersion","modelInstanceId":4685,"modelId":2820},{"sourceId":6065,"sourceType":"modelInstanceVersion","modelInstanceId":4686,"modelId":2820},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689,"modelId":2821},{"sourceId":6069,"sourceType":"modelInstanceVersion","modelInstanceId":4690,"modelId":2821}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":3501.988572,"end_time":"2024-02-16T08:45:14.113133","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-16T07:46:52.124561","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"01e59add54544a4aa02f8596b0dc52c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e78327d156dd4616b1dce212ef23344b","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98119cc097024f49a1d784d9df3681e0","value":12}},"0fe191be05044e81b88db21604282614":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38e7f1eab668499fbc49911e97eddb1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0b0c5b771fa47e6997b0dcfc36eea1b","IPY_MODEL_f464cb26e2f24e7085e0053116244b42","IPY_MODEL_75badc6e7ccc4756b3c4fd5a4d0cc2c7"],"layout":"IPY_MODEL_b4a1889e5990485cb7053a34f0d20b29"}},"44accd5b6b874ce586c5f0bef90e7739":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2c6ad3c1a7748ee805622a3e945bc86","placeholder":"â€‹","style":"IPY_MODEL_c686017734dc4f31ad1ba8817b0ae164","value":"100%"}},"459155967072424cb23ea889339d9b7b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5156cd2a5e2347d6be09cceceef0c858":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bfc933c4a7040148f9fab7f799bf8d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fd237f1b7344371bc44d09c7e2c3a03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63be5218546e45f694f0f3298d5a8a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fd237f1b7344371bc44d09c7e2c3a03","placeholder":"â€‹","style":"IPY_MODEL_8cdbc8555e5148178faf9cf02aecab8a","value":"100%"}},"6ca3ef0da63c4c1789d9d9e8c3cb80bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7144494693304342a027632f3585f7c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75badc6e7ccc4756b3c4fd5a4d0cc2c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89ad24ab3ca445419fc60253436aa0c9","placeholder":"â€‹","style":"IPY_MODEL_82347cb45db245fb957d76c610c5828d","value":" 10/10 [00:00&lt;00:00, 688.08it/s]"}},"82347cb45db245fb957d76c610c5828d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89ad24ab3ca445419fc60253436aa0c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89dde074e4e146988f898fd9530ff8c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8cdbc8555e5148178faf9cf02aecab8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98119cc097024f49a1d784d9df3681e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a746c75095d4ab1b3ae3ede5544e3c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b2c6ad3c1a7748ee805622a3e945bc86":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4a1889e5990485cb7053a34f0d20b29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7f5466713824bc8b1e102d6b14863bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63be5218546e45f694f0f3298d5a8a05","IPY_MODEL_d0ce6fb4c388445aa7b8d35ac1b94722","IPY_MODEL_cd0f9804b7894fd4be910dd3c9123321"],"layout":"IPY_MODEL_6ca3ef0da63c4c1789d9d9e8c3cb80bc"}},"c686017734dc4f31ad1ba8817b0ae164":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c68aa6e5241a4a6aa4d89817883d7700":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd0f9804b7894fd4be910dd3c9123321":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f20cbd5f0dde42d38cc567f164fde9f6","placeholder":"â€‹","style":"IPY_MODEL_7144494693304342a027632f3585f7c4","value":" 6807/6807 [00:03&lt;00:00, 2271.35it/s]"}},"d0b0c5b771fa47e6997b0dcfc36eea1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fe191be05044e81b88db21604282614","placeholder":"â€‹","style":"IPY_MODEL_f7675d631464480fa57ba24e2eeeb226","value":"100%"}},"d0ce6fb4c388445aa7b8d35ac1b94722":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bfc933c4a7040148f9fab7f799bf8d9","max":6807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a746c75095d4ab1b3ae3ede5544e3c6","value":6807}},"d9c516b12ef1443f9a05c184e6151e24":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e78327d156dd4616b1dce212ef23344b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee5d642062a040a4b35aabb3336774a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44accd5b6b874ce586c5f0bef90e7739","IPY_MODEL_01e59add54544a4aa02f8596b0dc52c3","IPY_MODEL_f3fd50ac145f4fa8a707c3010c01a694"],"layout":"IPY_MODEL_459155967072424cb23ea889339d9b7b"}},"f20cbd5f0dde42d38cc567f164fde9f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3fd50ac145f4fa8a707c3010c01a694":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c68aa6e5241a4a6aa4d89817883d7700","placeholder":"â€‹","style":"IPY_MODEL_d9c516b12ef1443f9a05c184e6151e24","value":" 12/12 [00:00&lt;00:00, 571.99it/s]"}},"f464cb26e2f24e7085e0053116244b42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5156cd2a5e2347d6be09cceceef0c858","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89dde074e4e146988f898fd9530ff8c3","value":10}},"f7675d631464480fa57ba24e2eeeb226":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ“š | Import Libraries ","metadata":{"papermill":{"duration":0.016722,"end_time":"2024-02-16T07:48:45.596153","exception":false,"start_time":"2024-02-16T07:48:45.579431","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" # # you can also use tensorflow or torch\n\nimport keras\nimport keras_nlp\nfrom keras import ops\nimport tensorflow as tf\n\nimport json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n\nimport plotly.graph_objs as go\nimport plotly.express as px","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-16T07:48:45.631895Z","iopub.status.busy":"2024-02-16T07:48:45.631214Z","iopub.status.idle":"2024-02-16T07:49:00.079114Z","shell.execute_reply":"2024-02-16T07:49:00.078003Z"},"papermill":{"duration":14.468797,"end_time":"2024-02-16T07:49:00.081872","exception":false,"start_time":"2024-02-16T07:48:45.613075","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Library Versions","metadata":{"papermill":{"duration":0.016927,"end_time":"2024-02-16T07:49:00.116577","exception":false,"start_time":"2024-02-16T07:49:00.09965","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasNLP:\", keras_nlp.__version__)","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-16T07:49:00.152972Z","iopub.status.busy":"2024-02-16T07:49:00.151796Z","iopub.status.idle":"2024-02-16T07:49:00.157907Z","shell.execute_reply":"2024-02-16T07:49:00.156963Z"},"papermill":{"duration":0.026836,"end_time":"2024-02-16T07:49:00.160458","exception":false,"start_time":"2024-02-16T07:49:00.133622","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# âš™ï¸ | Configuration","metadata":{"papermill":{"duration":0.016631,"end_time":"2024-02-16T07:49:00.195286","exception":false,"start_time":"2024-02-16T07:49:00.178655","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    preset = \"deberta_v3_small_en\" # name of pretrained backbone\n    train_seq_len = 1024 # max size of input sequence for training\n    train_batch_size = 2 * 8 # size of the input batch in training, x 2 as two GPUs\n    infer_seq_len = 2000 # max size of input sequence for inference\n    infer_batch_size = 2 * 2 # size of the input batch in inference, x 2 as two GPUs\n    epochs = 6 # number of epochs to train\n    lr_mode = \"exp\" # lr scheduler mode from one of \"cos\", \"step\", \"exp\"\n    \n    labels = [\"B-EMAIL\", \"B-ID_NUM\", \"B-NAME_STUDENT\", \"B-PHONE_NUM\",\n              \"B-STREET_ADDRESS\", \"B-URL_PERSONAL\", \"B-USERNAME\",\n              \"I-ID_NUM\", \"I-NAME_STUDENT\", \"I-PHONE_NUM\",\n              \"I-STREET_ADDRESS\",\"I-URL_PERSONAL\",\"O\"]\n    id2label = dict(enumerate(labels)) # integer label to BIO format label mapping\n    label2id = {v:k for k,v in id2label.items()} # BIO format label to integer label mapping\n    num_labels = len(labels) # number of PII (NER) tags\n    \n    train = True # whether to train or use already trained ckpt","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:00.232044Z","iopub.status.busy":"2024-02-16T07:49:00.231681Z","iopub.status.idle":"2024-02-16T07:49:00.239209Z","shell.execute_reply":"2024-02-16T07:49:00.238207Z"},"papermill":{"duration":0.027915,"end_time":"2024-02-16T07:49:00.241295","exception":false,"start_time":"2024-02-16T07:49:00.21338","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# â™»ï¸ | Reproducibility \nSets value for random seed to produce similar result in each run.","metadata":{"papermill":{"duration":0.01713,"end_time":"2024-02-16T07:49:00.275625","exception":false,"start_time":"2024-02-16T07:49:00.258495","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:00.311725Z","iopub.status.busy":"2024-02-16T07:49:00.311049Z","iopub.status.idle":"2024-02-16T07:49:00.31604Z","shell.execute_reply":"2024-02-16T07:49:00.315071Z"},"papermill":{"duration":0.02544,"end_time":"2024-02-16T07:49:00.318192","exception":false,"start_time":"2024-02-16T07:49:00.292752","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸš€ | Distributed Training / Inference\n\nIn this notebook, we will also use the `Data Parallel` strategy for **Distributed Training/Inference**. This means that the model weights will be replicated across all devices, and each device will process a portion of the input data.\n\n> **Note**: Currently, `DataParallel` is implemented on the JAX backend, so for TensorFlow and PyTorch backends, we can only use a single GPU.","metadata":{"papermill":{"duration":0.016534,"end_time":"2024-02-16T07:49:00.352055","exception":false,"start_time":"2024-02-16T07:49:00.335521","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Get devices default \"gpu\" or \"tpu\"\ndevices = keras.distribution.list_devices()\nprint(\"Device:\", devices)\n\nif len(devices) > 1:\n    # Data parallelism\n    data_parallel = keras.distribution.DataParallel(devices=devices)\n\n    # Set the global distribution.\n    keras.distribution.set_distribution(data_parallel)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:00.387527Z","iopub.status.busy":"2024-02-16T07:49:00.387185Z","iopub.status.idle":"2024-02-16T07:49:01.43378Z","shell.execute_reply":"2024-02-16T07:49:01.432688Z"},"papermill":{"duration":1.067016,"end_time":"2024-02-16T07:49:01.435995","exception":false,"start_time":"2024-02-16T07:49:00.368979","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ§® | Mixed Precision\n\nTo enable larger batch sizes and faster training, we'll utilize `mixed_precision` in this notebook. In Keras, this can be achieved with just **one line of code**, as shown below.","metadata":{"papermill":{"duration":0.017593,"end_time":"2024-02-16T07:49:01.474376","exception":false,"start_time":"2024-02-16T07:49:01.456783","status":"completed"},"tags":[]}},{"cell_type":"code","source":"keras.mixed_precision.set_global_policy(\"mixed_float16\") ","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:01.513098Z","iopub.status.busy":"2024-02-16T07:49:01.51199Z","iopub.status.idle":"2024-02-16T07:49:01.517872Z","shell.execute_reply":"2024-02-16T07:49:01.516798Z"},"papermill":{"duration":0.028026,"end_time":"2024-02-16T07:49:01.520199","exception":false,"start_time":"2024-02-16T07:49:01.492173","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“ | Dataset Path ","metadata":{"papermill":{"duration":0.017572,"end_time":"2024-02-16T07:49:01.555856","exception":false,"start_time":"2024-02-16T07:49:01.538284","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/pii-detection-removal-from-educational-data\"","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:01.594763Z","iopub.status.busy":"2024-02-16T07:49:01.593874Z","iopub.status.idle":"2024-02-16T07:49:01.599272Z","shell.execute_reply":"2024-02-16T07:49:01.598189Z"},"papermill":{"duration":0.027378,"end_time":"2024-02-16T07:49:01.601476","exception":false,"start_time":"2024-02-16T07:49:01.574098","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“– | Meta Data\n\nThe competition dataset contains ~$22,000$ student essays where $70\\%$ essays are reserved for **testing**, leaving $30\\%$ for **training** and **validation**.\n\nSure, here's the modified markdown with an example of the BIO format label:\n\n**Data Overview:**\n\n* All essays were written in response to the **same prompt**, applying course material to a real-world problem.\n* The dataset includes **7 types of PII**: `NAME_STUDENT`, `EMAIL`, `USERNAME`, `ID_NUM`, `PHONE_NUM`, `URL_PERSONAL`, `STREET_ADDRESS`.\n* Labels are given in **BIO (Beginning, Inner, Outer)** format.\n\n**Example of BIO format label:**\n\nLet's consider a sentence: `\"The email address of Michael jordan is mjordan@nba.com\"`. In BIO format, the labels for the personally identifiable information (PII) would be annotated as follows:\n\n| **Word** | The | email | address | of | Michael | Jordan | is | mjordan@nba.com |\n|----------|-----|-------|---------|----|---------|--------|----|----------------|\n| **Label** | O   | O     | O       | O  | B-NAME_STUDENT | I-NAME_STUDENT | O  | B-EMAIL        |\n\nIn the example above, `B-` indicates the beginning of an PII, `I-` indicates an inner part of a multi-token PII, and `O` indicates tokens that do not belong to any PII.\n\n**Data Format:**\n\n* The train/test data is stored in `{test|train}.json` files.\n* Each json file has:\n    * `document`: unique ID (integer)\n    * `full_text`: essay content (string)\n    * `tokens`: individual words in the essay (list of strings)\n    * `labels` (training data only): BIO labels for each token (list of strings)","metadata":{"papermill":{"duration":0.017478,"end_time":"2024-02-16T07:49:01.640744","exception":false,"start_time":"2024-02-16T07:49:01.623266","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Train-Valid data\ndata = json.load(open(f\"{BASE_PATH}/train.json\"))\n\n# Initialize empty arrays\nwords = np.empty(len(data), dtype=object)\nlabels = np.empty(len(data), dtype=object)\n\n# Fill the arrays\nfor i, x in tqdm(enumerate(data), total=len(data)):\n    words[i] = np.array(x[\"tokens\"])\n    labels[i] = np.array([CFG.label2id[label] for label in x[\"labels\"]])","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:01.681549Z","iopub.status.busy":"2024-02-16T07:49:01.68045Z","iopub.status.idle":"2024-02-16T07:49:07.824477Z","shell.execute_reply":"2024-02-16T07:49:07.823421Z"},"papermill":{"duration":6.169074,"end_time":"2024-02-16T07:49:07.827688","exception":false,"start_time":"2024-02-16T07:49:01.658614","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“Š | Exploratory Data Analysis\n\nFrom the following label distribution plot, it is evident that there is a significant **class imbalance** between PII tags. This could be a key area for improvement where **external datasets** and **augmentations** could play a pivotal role.","metadata":{"papermill":{"duration":0.066564,"end_time":"2024-02-16T07:49:07.912259","exception":false,"start_time":"2024-02-16T07:49:07.845695","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Get unique labels and their frequency\nall_labels = np.array([x for label in labels for x in label])\nunique_labels, label_counts = np.unique(all_labels, return_counts=True)\n\n# Plotting\nfig = go.Figure(data=go.Bar(x=CFG.labels, y=label_counts))\nfig.update_layout(\n    title=\"Label Distribution\",\n    xaxis_title=\"Labels\",\n    yaxis_title=\"Count\",\n    yaxis_type=\"log\",\n)\n\nfig.update_traces(text=label_counts, textposition=\"outside\")\nfig.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-16T07:49:07.949478Z","iopub.status.busy":"2024-02-16T07:49:07.949047Z","iopub.status.idle":"2024-02-16T07:49:09.636577Z","shell.execute_reply":"2024-02-16T07:49:09.635546Z"},"papermill":{"duration":1.709122,"end_time":"2024-02-16T07:49:09.639083","exception":false,"start_time":"2024-02-16T07:49:07.929961","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ”ª | Data Split\n\nIn the following code snippet, we will split the dataset into training and testing subsets using an `80%-20%` ratio.","metadata":{"papermill":{"duration":0.017994,"end_time":"2024-02-16T07:49:09.676329","exception":false,"start_time":"2024-02-16T07:49:09.658335","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Splitting the data into training and testing sets\ntrain_words, valid_words, train_labels, valid_labels = train_test_split(\n    words, labels, test_size=0.2, random_state=CFG.seed\n)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:09.715289Z","iopub.status.busy":"2024-02-16T07:49:09.714397Z","iopub.status.idle":"2024-02-16T07:49:09.721798Z","shell.execute_reply":"2024-02-16T07:49:09.720766Z"},"papermill":{"duration":0.029576,"end_time":"2024-02-16T07:49:09.724196","exception":false,"start_time":"2024-02-16T07:49:09.69462","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ½ï¸ | Pre-Processing\n\nInitially, raw text data is quite complex and challenging for modeling due to its high dimensionality. We simplify this complexity by converting text into words then more manageable set of tokens using `tokenizers`. For example, transforming the sentence `\"The quick brown fox\"` into tokens like `[\"the\", \"qu\", \"##ick\", \"br\", \"##own\", \"fox\"]` helps us break down the text effectively. Then, since models can't directly process strings, they are converted into integers, like `[10, 23, 40, 51, 90, 84]`. Additionally, many models require special tokens and additional tensors to understand input better. A `preprocessing` layer helps with this by adding these special tokens, which aid in separating input and identifying padding, among other tasks.\n\nYou can explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:\n- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)\n- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)","metadata":{"papermill":{"duration":0.018262,"end_time":"2024-02-16T07:49:09.762819","exception":false,"start_time":"2024-02-16T07:49:09.744557","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# To convert string input or list of strings input to numerical tokens\ntokenizer = keras_nlp.models.DebertaV3Tokenizer.from_preset(\n    CFG.preset,\n)\n\n# Preprocessing layer to add spetical tokens: [CLS], [SEP], [PAD]\npacker = keras_nlp.layers.MultiSegmentPacker(\n    start_value=tokenizer.cls_token_id,\n    end_value=tokenizer.sep_token_id,\n    sequence_length=10,\n)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:09.801183Z","iopub.status.busy":"2024-02-16T07:49:09.800809Z","iopub.status.idle":"2024-02-16T07:49:11.557976Z","shell.execute_reply":"2024-02-16T07:49:11.557053Z"},"papermill":{"duration":1.779222,"end_time":"2024-02-16T07:49:11.560444","exception":false,"start_time":"2024-02-16T07:49:09.781222","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer in Action\n\nThe following code shows the effects of `DebertaV3Tokenizer`. We can see that the word `[\"reflexion\"]` has been divided into `[\"â–reflex\", \"ion\"]` tokens. Therefore, for similar cases, it's necessary to align labels of tokens to labels of words.","metadata":{"papermill":{"duration":0.018103,"end_time":"2024-02-16T07:49:11.59705","exception":false,"start_time":"2024-02-16T07:49:11.578947","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sample_words = words[0][:5]\nsample_tokens_int = [\n    token.tolist() for word in sample_words for token in tokenizer(word)\n]\nsample_tokens_str = [tokenizer.id_to_token(token) for token in sample_tokens_int]\n\nprint(\"words        :\", sample_words.tolist())\nprint(\"tokens (str) :\", sample_tokens_str)\nprint(\"tokens (int) :\", sample_tokens_int)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-16T07:49:11.635545Z","iopub.status.busy":"2024-02-16T07:49:11.635143Z","iopub.status.idle":"2024-02-16T07:49:13.351541Z","shell.execute_reply":"2024-02-16T07:49:13.350323Z"},"papermill":{"duration":1.738242,"end_time":"2024-02-16T07:49:13.353893","exception":false,"start_time":"2024-02-16T07:49:11.615651","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessor in Action\n\nEven though we converted string inputs to integer tokens with Tokenizer, we are not done yet. We need to add special tokens like `[CLS]`, `[SEP]`, `[PAD]`. This is wehere `Preprocessing` layer comes into the picture. In this notebook, we will use `MultiSegmentPacker` layer. Let's see it action.","metadata":{"papermill":{"duration":0.018244,"end_time":"2024-02-16T07:49:13.391197","exception":false,"start_time":"2024-02-16T07:49:13.372953","status":"completed"},"tags":[]}},{"cell_type":"code","source":"padded_sample_tokens_int = packer(np.array(sample_tokens_int))[0].tolist()\npadded_sample_tokens_str = [\n    tokenizer.id_to_token(token) for token in padded_sample_tokens_int\n]\n\nprint(\"tokens (str)        :\", sample_tokens_str)\nprint(\"padded tokens (str) :\", padded_sample_tokens_str, \"\\n\")\n\nprint(\"tokens (int)        :\", sample_tokens_int)\nprint(\"padded tokens (int) :\", padded_sample_tokens_int)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-16T07:49:13.429189Z","iopub.status.busy":"2024-02-16T07:49:13.428409Z","iopub.status.idle":"2024-02-16T07:49:13.535035Z","shell.execute_reply":"2024-02-16T07:49:13.533869Z"},"papermill":{"duration":0.128149,"end_time":"2024-02-16T07:49:13.537411","exception":false,"start_time":"2024-02-16T07:49:13.409262","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ¥£ | Data Processing\n\nOne of the key factors that sets Token Classification apart from Text Classification is the data processing part. Unlike text classification, where we simply send our tokenized text to the model, in token classification, we have to apply more processing before sending it to the model. For example, when the `tokenizer` creates multiple tokens for single word or the `processing` layer adds special tokens `[CLS]`, `[SEP]`, and `[PAD]`, they create a mismatch between the input and labels. Thus, a single word corresponding to a single label may now be split into two tokens. We need to realign the tokens labels with word labels by:\n\n- Mapping tokens label to their corresponding word label using `token_ids`.\n- Assigning the label `-100` to special tokens `[CLS]`, `[SEP]` and `[PAD]` to disregard them in the `CrossEntropy` loss calculation.\n- Labeling only the first token of each word and assigning `-100` to other tokens belonging to the same word.\n\nSpecifically, the following cell contains the following functions:\n- `process_data()` - prepares input, label, and token_ids\n    - `get_tokens()` - creates input tokens ans padding masks from string words\n    - `get_token_ids()` - generates token ids for aligning tokens and labels\n    - `get_token_labels()` - realigns token labels and adds padding (`-100`) to match input\n    - `process_token_ids()` - adds padding (`-1`) to token ids to match input","metadata":{"papermill":{"duration":0.018407,"end_time":"2024-02-16T07:49:13.575144","exception":false,"start_time":"2024-02-16T07:49:13.556737","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_tokens(words, seq_len, packer):\n    # Tokenize input\n    token_words = tf.expand_dims(\n        tokenizer(words), axis=-1\n    )  # ex: (words) [\"It's\", \"a\", \"cat\"] ->  (token_words) [[1, 2], [3], [4]]\n    tokens = tf.reshape(\n        token_words, [-1]\n    )  # ex: (token_words) [[1, 2], [3], [4]] -> (tokens) [1, 2, 3, 4]\n    # Pad tokens\n    tokens = packer(tokens)[0][:seq_len]\n    inputs = {\"token_ids\": tokens, \"padding_mask\": tokens != 0}\n    return inputs, tokens, token_words\n\n\ndef get_token_ids(token_words):\n    # Get word indices\n    word_ids = tf.range(tf.shape(token_words)[0])\n    # Get size of each word\n    word_size = tf.reshape(tf.map_fn(lambda word: tf.shape(word)[0:1], token_words), [-1])\n    # Repeat word_id with size of word to get token_id\n    token_ids = tf.repeat(word_ids, word_size)\n    return token_ids\n\n\ndef get_token_labels(word_labels, token_ids, seq_len):\n    # Create token_labels from word_labels ->  alignment\n    token_labels = tf.gather(word_labels, token_ids)\n    # Only label the first token of a given word and assign -100 to others\n    mask = tf.concat([[True], token_ids[1:] != token_ids[:-1]], axis=0)\n    token_labels = tf.where(mask, token_labels, -100)\n    # Truncate to max sequence length\n    token_labels = token_labels[: seq_len - 2]  # -2 for special tokens ([CLS], [SEP])\n    # Pad token_labels to align with tokens (use -100 to pad for loss/metric ignore)\n    pad_start = 1  # for [CLS] token\n    pad_end = seq_len - tf.shape(token_labels)[0] - 1  # for [SEP] and [PAD] tokens\n    token_labels = tf.pad(token_labels, [[pad_start, pad_end]], constant_values=-100)\n    return token_labels\n\n\ndef process_token_ids(token_ids, seq_len):\n    # Truncate to max sequence length\n    token_ids = token_ids[: seq_len - 2]  # -2 for special tokens ([CLS], [SEP])\n    # Pad token_ids to align with tokens (use -1 to pad for later identification)\n    pad_start = 1  # [CLS] token\n    pad_end = seq_len - tf.shape(token_ids)[0] - 1  # [SEP] and [PAD] tokens\n    token_ids = tf.pad(token_ids, [[pad_start, pad_end]], constant_values=-1)\n    return token_ids\n\n\ndef process_data(seq_len=720, has_label=True, return_ids=False):\n    # To add spetical tokens: [CLS], [SEP], [PAD]\n    packer = keras_nlp.layers.MultiSegmentPacker(\n        start_value=tokenizer.cls_token_id,\n        end_value=tokenizer.sep_token_id,\n        sequence_length=seq_len,\n    )\n\n    def process(x):\n        # Generate inputs from tokens\n        inputs, tokens, words_int = get_tokens(x[\"words\"], seq_len, packer)\n        # Generate token_ids for maping tokens to words\n        token_ids = get_token_ids(words_int)\n        if has_label:\n            # Generate token_labels from word_labels\n            token_labels = get_token_labels(x[\"labels\"], token_ids, seq_len)\n            return inputs, token_labels\n        elif return_ids:\n            # Pad token_ids to align with tokens\n            token_ids = process_token_ids(token_ids, seq_len)\n            return token_ids\n        else:\n            return inputs\n\n    return process","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:13.613629Z","iopub.status.busy":"2024-02-16T07:49:13.613257Z","iopub.status.idle":"2024-02-16T07:49:13.629113Z","shell.execute_reply":"2024-02-16T07:49:13.628088Z"},"papermill":{"duration":0.037443,"end_time":"2024-02-16T07:49:13.631307","exception":false,"start_time":"2024-02-16T07:49:13.593864","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸš | Dataloader\n\nThe code below sets up a data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences. To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).\n\n> **Note**: We have used `ragged` tensor as each row has text with different sizes.","metadata":{"papermill":{"duration":0.01857,"end_time":"2024-02-16T07:49:13.668898","exception":false,"start_time":"2024-02-16T07:49:13.650328","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def build_dataset(words, labels=None, return_ids=False, batch_size=4,\n                  seq_len=512, shuffle=False, cache=True, drop_remainder=True):\n    AUTO = tf.data.AUTOTUNE \n\n    slices = {\"words\": tf.ragged.constant(words)}\n    if labels is not None:\n        slices.update({\"labels\": tf.ragged.constant(labels)})\n\n    ds = tf.data.Dataset.from_tensor_slices(slices)\n    ds = ds.map(process_data(seq_len=seq_len,\n                             has_label=labels is not None, \n                             return_ids=return_ids), num_parallel_calls=AUTO) # apply processing\n    ds = ds.cache() if cache else ds  # cache dataset\n    if shuffle: # shuffle dataset\n        ds = ds.shuffle(1024, seed=CFG.seed)  \n        opt = tf.data.Options() \n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # batch dataset\n    ds = ds.prefetch(AUTO)  # prefetch next batch\n    return ds","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:13.707993Z","iopub.status.busy":"2024-02-16T07:49:13.707593Z","iopub.status.idle":"2024-02-16T07:49:13.716696Z","shell.execute_reply":"2024-02-16T07:49:13.715702Z"},"papermill":{"duration":0.031224,"end_time":"2024-02-16T07:49:13.71894","exception":false,"start_time":"2024-02-16T07:49:13.687716","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Train & Valid Dataloader\n\nIn the following code, we'll create **train** and **valid** data loaders.","metadata":{"papermill":{"duration":0.018559,"end_time":"2024-02-16T07:49:13.75608","exception":false,"start_time":"2024-02-16T07:49:13.737521","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_ds = build_dataset(train_words, train_labels,  batch_size=CFG.train_batch_size,\n                         seq_len=CFG.train_seq_len, shuffle=True)\n\nvalid_ds = build_dataset(valid_words, valid_labels, batch_size=CFG.train_batch_size, \n                         seq_len=CFG.train_seq_len, shuffle=False)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:13.794928Z","iopub.status.busy":"2024-02-16T07:49:13.794591Z","iopub.status.idle":"2024-02-16T07:49:49.985809Z","shell.execute_reply":"2024-02-16T07:49:49.98485Z"},"papermill":{"duration":36.21361,"end_time":"2024-02-16T07:49:49.988542","exception":false,"start_time":"2024-02-16T07:49:13.774932","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Check\n\nLet's check a batch of samples and their associated labels from the dataset.","metadata":{"papermill":{"duration":0.018956,"end_time":"2024-02-16T07:49:50.027162","exception":false,"start_time":"2024-02-16T07:49:50.008206","status":"completed"},"tags":[]}},{"cell_type":"code","source":"inp, tar = next(iter(valid_ds))\nprint(\"# Input:\\n\",inp); print(\"\\n# Labels:\\n\",tar)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:50.066611Z","iopub.status.busy":"2024-02-16T07:49:50.066199Z","iopub.status.idle":"2024-02-16T07:49:50.757792Z","shell.execute_reply":"2024-02-16T07:49:50.75681Z"},"papermill":{"duration":0.714882,"end_time":"2024-02-16T07:49:50.760788","exception":false,"start_time":"2024-02-16T07:49:50.045906","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ” | Loss & Metric","metadata":{"papermill":{"duration":0.017878,"end_time":"2024-02-16T07:49:50.798861","exception":false,"start_time":"2024-02-16T07:49:50.780983","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Loss: CrossEntropy\n\nTo optimize our model we will use `CrossEntropy` loss, also known as log loss. It is defined as:\n\n$$\n\\text{CrossEntropy} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n$$\n\nWhere:\n- $N$ is the number of samples.\n- $y_i$ is the true label of the $i^{th}$ sample.\n- $\\hat{y}_i$ is the predicted probability of the $i^{th}$ sample being in the positive class.\n\n> **Note**: We will not compute loss for `ignore_class` which indicates special tokens (`[CLS]`, `[SEP]`, `[PAD]`) or intermediate token of a word.","metadata":{"papermill":{"duration":0.01818,"end_time":"2024-02-16T07:49:50.835681","exception":false,"start_time":"2024-02-16T07:49:50.817501","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CrossEntropy(keras.losses.SparseCategoricalCrossentropy):\n    def __init__(self, ignore_class=-100, reduction=None, **args):\n        super().__init__(reduction=reduction, **args)\n        self.ignore_class = ignore_class\n\n    def call(self, y_true, y_pred):\n        y_true = ops.reshape(y_true, [-1])\n        y_pred = ops.reshape(y_pred, [-1, CFG.num_labels])\n        loss = super().call(y_true, y_pred)\n        if self.ignore_class is not None:\n            valid_mask = ops.not_equal(\n                y_true, ops.cast(self.ignore_class, y_pred.dtype)\n            )\n            loss = ops.where(valid_mask, loss, 0.0)\n            loss = ops.sum(loss)\n            loss /= ops.maximum(ops.sum(ops.cast(valid_mask, loss.dtype)), 1)\n        else:\n            loss = ops.mean(loss)\n        return loss\n","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:50.874342Z","iopub.status.busy":"2024-02-16T07:49:50.873536Z","iopub.status.idle":"2024-02-16T07:49:50.882323Z","shell.execute_reply":"2024-02-16T07:49:50.881414Z"},"papermill":{"duration":0.030599,"end_time":"2024-02-16T07:49:50.884525","exception":false,"start_time":"2024-02-16T07:49:50.853926","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metric: FBetaScore ($\\beta = 5$)\n\nThe competition metric is $F^\\beta$, which combines precision and recall, weighted by a parameter $\\beta = 5$. It is defined as:\n\n$$\n\\text{FBetaScore} = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}\n$$\n\nWhere:\n- Precision $= \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$\n- Recall $= \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$\n- $\\beta$ controls the weighting between precision and recall. As in this competition, $\\beta = 5$, it means more weight is given to recall. In other words, **metric will penalize more, if a positive token is classified as negative**.\n\n> **Note${}^1$**: The competition will use `micro` averaging for the `FBetaScore`, considering total counts across all classes, which is influenced by class imbalances. The `macro` averaging treats each class equally, regardless of frequency. Organizers may want models that perform well on predicting more common PII tags.\n\n> **Note${}^2$**: We will not compute the metric for `ignore_classes`, which indicates special tokens (`[CLS]`, `[SEP]`, `[PAD]`) or non-start tokens of a word or `O` (Outer) labels.","metadata":{"papermill":{"duration":0.018328,"end_time":"2024-02-16T07:49:50.921608","exception":false,"start_time":"2024-02-16T07:49:50.90328","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FBetaScore(keras.metrics.FBetaScore):\n    def __init__(self, ignore_classes=[-100, 12], average=\"micro\", beta=5.0,\n                 name=\"f5_score\", **args):\n        super().__init__(beta=beta, average=average, name=name, **args)\n        self.ignore_classes = ignore_classes or []\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = ops.convert_to_tensor(y_true, dtype=self.dtype)\n        y_pred = ops.convert_to_tensor(y_pred, dtype=self.dtype)\n        \n        y_true = ops.reshape(y_true, [-1])\n        y_pred = ops.reshape(y_pred, [-1, CFG.num_labels])\n            \n        valid_mask = ops.ones_like(y_true, dtype=self.dtype)\n        if self.ignore_classes:\n            for ignore_class in self.ignore_classes:\n                valid_mask &= ops.not_equal(y_true, ops.cast(ignore_class, y_pred.dtype))\n        valid_mask = ops.expand_dims(valid_mask, axis=-1)\n        \n        y_true = ops.one_hot(y_true, CFG.num_labels)\n        \n        if not self._built:\n            self._build(y_true.shape, y_pred.shape)\n\n        threshold = ops.max(y_pred, axis=-1, keepdims=True)\n        y_pred = ops.logical_and(\n            y_pred >= threshold, ops.abs(y_pred) > 1e-9\n        )\n\n        y_pred = ops.cast(y_pred, dtype=self.dtype)\n        y_true = ops.cast(y_true, dtype=self.dtype)\n        \n        tp = ops.sum(y_pred * y_true * valid_mask, self.axis)\n        fp = ops.sum(y_pred * (1 - y_true) * valid_mask, self.axis)\n        fn = ops.sum((1 - y_pred) * y_true * valid_mask, self.axis)\n            \n        self.true_positives.assign_add(tp)\n        self.false_positives.assign_add(fp)\n        self.false_negatives.assign_add(fn)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:50.960285Z","iopub.status.busy":"2024-02-16T07:49:50.959463Z","iopub.status.idle":"2024-02-16T07:49:50.973061Z","shell.execute_reply":"2024-02-16T07:49:50.972072Z"},"papermill":{"duration":0.035189,"end_time":"2024-02-16T07:49:50.975266","exception":false,"start_time":"2024-02-16T07:49:50.940077","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ¤– | Modeling\n\nIn this notebook, we will use the `DebertaV3` backbone from KerasNLP's pretrained models to extract features of tokens and employ `Dense` layers for token-level classification. Unlike Text Classification, transformer outputs are not pooled; instead, a `Dense` layer is applied to the outputs to obtain predictions.\n\nTo clarify, the output of the transformer model is a 3D tensor of shape $(batch\\_size, seq\\_len, feat\\_dim)$, where only the $feat\\_dim$ is changed, while the others remain the same. Subsequently, the `Dense` (or `Linear`) layer maps the `feat_dim` to `num_labels` and then applies a `softmax` activation to get the final prediction.\n\nTo explore other backbones, simply modify the `preset` in the `CFG` (config). A list of available pretrained backbones can be found on the [KerasNLP website](https://keras.io/api/keras_nlp/models/).\n\n> **Note:** The output `dtype` of the final activation is manually set to `float32` to facilitate `mixed_precision`.\n\n<u>Food for thought</u>: \n1. Some may wonder why the input to the `Dense` layer is 3D `(batch_size, d0, d1)` instead of the traditional 2D `(batch_size, d0)`. You can check [Hint: you can check this page](https://keras.io/api/layers/core_layers/dense/).\n2. We are training our model with sequence of `1024` length, however we are doing inference with sequence of `2000` length. What is happening here?","metadata":{"papermill":{"duration":0.018268,"end_time":"2024-02-16T07:49:51.012048","exception":false,"start_time":"2024-02-16T07:49:50.99378","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build Token Classification model\nbackbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n    CFG.preset,\n)\nout = backbone.output\nout = keras.layers.Dense(CFG.num_labels, name=\"logits\")(out)\nout = keras.layers.Activation(\"softmax\", dtype=\"float32\", name=\"prediction\")(out)\nmodel = keras.models.Model(backbone.input, out)\n\n# Compile model for optimizer, loss and metric\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=2e-5),\n    loss=CrossEntropy(),\n    metrics=[FBetaScore()],\n)\n\n# Summary of the model architecture\nmodel.summary()","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:49:51.050617Z","iopub.status.busy":"2024-02-16T07:49:51.050168Z","iopub.status.idle":"2024-02-16T07:50:03.237226Z","shell.execute_reply":"2024-02-16T07:50:03.236157Z"},"papermill":{"duration":12.209044,"end_time":"2024-02-16T07:50:03.239522","exception":false,"start_time":"2024-02-16T07:49:51.030478","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# âš“ | LR Schedule\n\nA well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.","metadata":{"papermill":{"duration":0.019999,"end_time":"2024-02-16T07:50:03.280284","exception":false,"start_time":"2024-02-16T07:50:03.260285","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import math\n\ndef get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n    lr_start, lr_max, lr_min = 6e-6, 2.5e-6 * batch_size, 1e-6\n    lr_ramp_ep, lr_sus_ep, lr_decay = 3, 0, 0.75\n\n    def lrfn(epoch):  # Learning rate update function\n        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n        elif mode == 'cos':\n            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n            phase = math.pi * decay_epoch_index / decay_total_epochs\n            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n        return lr\n\n    if plot:  # Plot lr curve if plot is True\n        fig = px.line(x=np.arange(epochs),\n                      y=[lrfn(epoch) for epoch in np.arange(epochs)], \n                      title='LR Scheduler',\n                      markers=True,\n                      labels={'x': 'epoch', 'y': 'lr'})\n        fig.update_layout(\n            yaxis = dict(\n                showexponent = 'all',\n                exponentformat = 'e'\n            )\n        )\n        fig.show()\n\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-16T07:50:03.322924Z","iopub.status.busy":"2024-02-16T07:50:03.322546Z","iopub.status.idle":"2024-02-16T07:50:03.334347Z","shell.execute_reply":"2024-02-16T07:50:03.333367Z"},"papermill":{"duration":0.035824,"end_time":"2024-02-16T07:50:03.336425","exception":false,"start_time":"2024-02-16T07:50:03.300601","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_cb = get_lr_callback(CFG.train_batch_size, mode=CFG.lr_mode, plot=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2024-02-16T07:50:03.378406Z","iopub.status.busy":"2024-02-16T07:50:03.378023Z","iopub.status.idle":"2024-02-16T07:50:05.538089Z","shell.execute_reply":"2024-02-16T07:50:05.537048Z"},"papermill":{"duration":2.183851,"end_time":"2024-02-16T07:50:05.54058","exception":false,"start_time":"2024-02-16T07:50:03.356729","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸš‚ | Training","metadata":{"papermill":{"duration":0.020625,"end_time":"2024-02-16T07:50:05.582176","exception":false,"start_time":"2024-02-16T07:50:05.561551","status":"completed"},"tags":[]}},{"cell_type":"code","source":"if CFG.train:\n    history = model.fit(\n        train_ds,\n        validation_data=valid_ds,\n        epochs=CFG.epochs,\n        callbacks=[lr_cb],\n        verbose=1,\n    )\nelse:\n    model.load_weights(\"/kaggle/input/pii-data-detection-ckpt-ds/model.weights.h5\")","metadata":{"execution":{"iopub.execute_input":"2024-02-16T07:50:05.624776Z","iopub.status.busy":"2024-02-16T07:50:05.624389Z","iopub.status.idle":"2024-02-16T08:42:07.469322Z","shell.execute_reply":"2024-02-16T08:42:07.468262Z"},"papermill":{"duration":3122.142451,"end_time":"2024-02-16T08:42:07.745086","exception":false,"start_time":"2024-02-16T07:50:05.602635","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though this notebook does both training and inference, let's store the **weights** of the trained model on disk in case we may need it later. Also, if you need more time for inference, you can create separate notebooks for training and inference.\n\n> **Note**: The filename of the weights should end in `.weights.h5`","metadata":{"papermill":{"duration":0.20854,"end_time":"2024-02-16T08:42:08.160426","exception":false,"start_time":"2024-02-16T08:42:07.951886","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.save_weights(\"model.weights.h5\")","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:42:08.58557Z","iopub.status.busy":"2024-02-16T08:42:08.585139Z","iopub.status.idle":"2024-02-16T08:42:13.363193Z","shell.execute_reply":"2024-02-16T08:42:13.361799Z"},"papermill":{"duration":4.999668,"end_time":"2024-02-16T08:42:13.36729","exception":false,"start_time":"2024-02-16T08:42:08.367622","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”¬ | Evaluation\n\nWe have trained and validated our model on a `1024` sequence length; however, we will be making inference using a `2000` sequence length. Thus, it is important to check how our model performs with `2000` sequence length inputs.","metadata":{"papermill":{"duration":0.244243,"end_time":"2024-02-16T08:42:13.873344","exception":false,"start_time":"2024-02-16T08:42:13.629101","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Build Validation dataloader with \"infer_seq_len\"\nvalid_ds = build_dataset(valid_words, valid_labels, return_ids=False, batch_size=CFG.infer_batch_size,\n                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:42:14.347069Z","iopub.status.busy":"2024-02-16T08:42:14.346202Z","iopub.status.idle":"2024-02-16T08:42:21.638093Z","shell.execute_reply":"2024-02-16T08:42:21.637193Z"},"papermill":{"duration":7.518302,"end_time":"2024-02-16T08:42:21.640732","exception":false,"start_time":"2024-02-16T08:42:14.12243","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate\nmodel.evaluate(valid_ds, return_dict=True, verbose=0)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:42:22.515851Z","iopub.status.busy":"2024-02-16T08:42:22.515288Z","iopub.status.idle":"2024-02-16T08:44:55.790461Z","shell.execute_reply":"2024-02-16T08:44:55.7894Z"},"papermill":{"duration":153.749968,"end_time":"2024-02-16T08:44:56.009957","exception":false,"start_time":"2024-02-16T08:42:22.259989","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ§ª | Prediction","metadata":{"papermill":{"duration":0.216442,"end_time":"2024-02-16T08:44:56.456322","exception":false,"start_time":"2024-02-16T08:44:56.23988","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Build Test Dataloader","metadata":{"papermill":{"duration":0.209307,"end_time":"2024-02-16T08:44:56.875886","exception":false,"start_time":"2024-02-16T08:44:56.666579","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Test data\ntest_data = json.load(open(f\"{BASE_PATH}/test.json\"))\n\n# Ensure number of samples is divisble by number of devices\nneed_samples  = len(devices) - len(test_data) % len(devices)\nfor _ in range(need_samples):\n    test_data.append(test_data[-1]) # repeat the last sample\n    \n# Initialize empty arrays\ntest_words = np.empty(len(test_data), dtype=object)\ntest_docs = np.empty(len(test_data), dtype=np.int32)\n\n# Fill the arrays\nfor i, x in tqdm(enumerate(test_data), total=len(test_data)):\n    test_words[i] = np.array(x[\"tokens\"])\n    test_docs[i] = x[\"document\"]\n\n# Get token ids\nid_ds = build_dataset(test_words, return_ids=True, batch_size=len(test_words), \n                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False, drop_remainder=False)\ntest_token_ids = ops.convert_to_numpy([ids for ids in iter(id_ds)][0])\n\n# Build test dataloader\ntest_ds = build_dataset(test_words, return_ids=False, batch_size=CFG.infer_batch_size,\n                        seq_len=CFG.infer_seq_len, shuffle=False, cache=False, drop_remainder=False)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:44:57.29381Z","iopub.status.busy":"2024-02-16T08:44:57.292889Z","iopub.status.idle":"2024-02-16T08:44:59.213577Z","shell.execute_reply":"2024-02-16T08:44:59.212435Z"},"papermill":{"duration":2.130211,"end_time":"2024-02-16T08:44:59.216186","exception":false,"start_time":"2024-02-16T08:44:57.085975","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"papermill":{"duration":0.207925,"end_time":"2024-02-16T08:44:59.697439","exception":false,"start_time":"2024-02-16T08:44:59.489514","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Do inference\ntest_preds = model.predict(test_ds, verbose=1)\n\n# Convert probabilities to class labels via max confidence\ntest_preds = np.argmax(test_preds, axis=-1)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:45:00.127885Z","iopub.status.busy":"2024-02-16T08:45:00.126917Z","iopub.status.idle":"2024-02-16T08:45:06.269047Z","shell.execute_reply":"2024-02-16T08:45:06.268076Z"},"papermill":{"duration":6.363433,"end_time":"2024-02-16T08:45:06.271829","exception":false,"start_time":"2024-02-16T08:44:59.908396","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Extra Samples\n\nWe need to remove the extra samples we added to the test data to ensure the number of samples is divisible by the number of devices.","metadata":{"papermill":{"duration":0.212627,"end_time":"2024-02-16T08:45:06.711891","exception":false,"start_time":"2024-02-16T08:45:06.499264","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_docs = test_docs[:-need_samples]\ntest_token_ids = test_token_ids[:-need_samples]\ntest_preds = test_preds[:-need_samples]\ntest_words = test_words[:-need_samples]","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:45:07.132544Z","iopub.status.busy":"2024-02-16T08:45:07.131607Z","iopub.status.idle":"2024-02-16T08:45:07.137018Z","shell.execute_reply":"2024-02-16T08:45:07.136011Z"},"papermill":{"duration":0.218741,"end_time":"2024-02-16T08:45:07.139215","exception":false,"start_time":"2024-02-16T08:45:06.920474","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ§¹ | Post-Processing\n\nThe following code processes the prediction to filter out unwanted parts. Specifically, it does the following:\n\n1. It filters out any tokens of a word that are not at the start (refer to the [ðŸ¥£ | Data Processing](https://www.kaggle.com/code/awsaf49/pii-data-detection-kerasnlp-starter-notebook#%F0%9F%A5%A3-%7C-Data-Processing) section for more details).\n2. It removes samples labeled as `O` (BIO format), as the submission file requires only non-`O` samples.\n3. It ignores predictions for special tokens like `[CLS]`, `[SEP]`, and `[PAD]`.\n\n> **Note**: A unique feature of following post-processing is that it uses numpy vectorized operations to filter out predictions, making it very fast and efficient.","metadata":{"papermill":{"duration":0.215753,"end_time":"2024-02-16T08:45:07.56801","exception":false,"start_time":"2024-02-16T08:45:07.352257","status":"completed"},"tags":[]}},{"cell_type":"code","source":"document_list = []\ntoken_id_list = []\nlabel_id_list = []\ntoken_list = []\n\nfor doc, token_ids, preds, tokens in tqdm(\n    zip(test_docs, test_token_ids, test_preds, test_words), total=len(test_words)\n):\n    # Create mask for filtering\n    mask1 = np.concatenate(([True], token_ids[1:] != token_ids[:-1])) # ignore non-start tokens of a word\n    mask2 = (preds != 12) # ignore `O` (BIO format) label -> 12 (integer format) label\n    mask3 = (token_ids != -1)  # ignore [CLS], [SEP], and [PAD] tokens\n    mask = (mask1 & mask2 & mask3) # merge filters\n    \n    # Apply filter\n    token_ids = token_ids[mask]\n    preds = preds[mask]\n\n     # Store prediction if number of tokens is not zero\n    if len(token_ids):\n        token_list.extend(tokens[token_ids])\n        document_list.extend([doc] * len(token_ids))\n        token_id_list.extend(token_ids)\n        label_id_list.extend(preds)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:45:07.985077Z","iopub.status.busy":"2024-02-16T08:45:07.98427Z","iopub.status.idle":"2024-02-16T08:45:08.009407Z","shell.execute_reply":"2024-02-16T08:45:08.008242Z"},"papermill":{"duration":0.241756,"end_time":"2024-02-16T08:45:08.016468","exception":false,"start_time":"2024-02-16T08:45:07.774712","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ“© | Submission\n\nLet's build a dataframe from the predictions which will help us visually check if our model is predicting correctly or not. We also have to map **integer** labels to **string** BIO format labels.","metadata":{"papermill":{"duration":0.210311,"end_time":"2024-02-16T08:45:08.44128","exception":false,"start_time":"2024-02-16T08:45:08.230969","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pred_df = pd.DataFrame(\n    {\n        \"document\": document_list,\n        \"token\": token_id_list,\n        \"label_id\": label_id_list,\n        \"token_string\": token_list,\n    }\n)\npred_df = pred_df.rename_axis(\"row_id\").reset_index() # add `row_id` column\npred_df[\"label\"] = pred_df.label_id.map(CFG.id2label) # map integer label to BIO format label\npred_df.head(10)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:45:08.865419Z","iopub.status.busy":"2024-02-16T08:45:08.864436Z","iopub.status.idle":"2024-02-16T08:45:08.912162Z","shell.execute_reply":"2024-02-16T08:45:08.9111Z"},"papermill":{"duration":0.261321,"end_time":"2024-02-16T08:45:08.914733","exception":false,"start_time":"2024-02-16T08:45:08.653412","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In `submission.csv` we are excluding `token_string` and `label_id` from the columns as they are not part of submission file.","metadata":{"papermill":{"duration":0.210398,"end_time":"2024-02-16T08:45:09.356374","exception":false,"start_time":"2024-02-16T08:45:09.145976","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub_df = pred_df.drop(columns=[\"token_string\", \"label_id\"]) # remove extra columns\nsub_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2024-02-16T08:45:09.783426Z","iopub.status.busy":"2024-02-16T08:45:09.782632Z","iopub.status.idle":"2024-02-16T08:45:09.796164Z","shell.execute_reply":"2024-02-16T08:45:09.795277Z"},"papermill":{"duration":0.232145,"end_time":"2024-02-16T08:45:09.798481","exception":false,"start_time":"2024-02-16T08:45:09.566336","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}